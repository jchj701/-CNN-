{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'F:/Desktop/TF图集/flower_train_1/output/'\n",
    "\n",
    "L1 = []\n",
    "L2 = []\n",
    "L3 = []\n",
    "L4 = []\n",
    "L5 = []\n",
    "label_1 = []\n",
    "label_2 = []\n",
    "label_3 = []\n",
    "label_4 = []\n",
    "label_5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取（处理后64x64）图片路径\n",
    "#ratio，测试集比率\n",
    "def get_files(file_dir, ratio):\n",
    "    for file in os.listdir(file_dir + '/1'):\n",
    "        L1.append(file_dir + '/1/' +file)\n",
    "        label_1.append(0)\n",
    "    for file in os.listdir(file_dir + '/2'):\n",
    "        L2.append(file_dir + '/2/' +file)\n",
    "        label_2.append(1)\n",
    "    for file in os.listdir(file_dir + '/3'):\n",
    "        L3.append(file_dir + '/3/' +file)\n",
    "        label_3.append(2)\n",
    "    for file in os.listdir(file_dir + '/4'):\n",
    "        L4.append(file_dir + '/4/' +file)\n",
    "        label_4.append(3)\n",
    "    for file in os.listdir(file_dir + '/5'):\n",
    "        L5.append(file_dir + '/5/' +file)\n",
    "        label_5.append(4)\n",
    "    #hstack 水平叠堆数组\n",
    "    #均在数组中使用hstack\n",
    "    image_list = np.hstack((L1, L2, L3, L4, L5))\n",
    "    label_list = np.hstack((label_1, label_2, label_3, label_4, label_5))\n",
    "    \n",
    "    \n",
    "    #shuffle 打乱顺序\n",
    "    order1 = np.array([image_list, label_list])\n",
    "    #transpose 转置\n",
    "    order2 = order1.transpose()\n",
    "    np.random.shuffle(order2)\n",
    "    \n",
    "    print(order1)\n",
    "    print(order2)\n",
    "#    print(disorder)\n",
    "    for x in L1():\n",
    "        print(x)\n",
    "    #disorder转list\n",
    "    image_list_turn = list(order2[:,0])\n",
    "    label_list_turn = list(order2[:,1])\n",
    "    \n",
    "    #划分list为test & train\n",
    "    n_sample = len(label_list_turn)\n",
    "    n_test = int(math.ceil(n_sample * ratio))\n",
    "    n_train = n_sample - n_test\n",
    "    \n",
    "    train_images = image_list_turn[0:n_train]\n",
    "    train_labels = label_list_turn[0:n_train]\n",
    "    train_labels = [int(float(i)) for i in train_labels]\n",
    "    test_images = image_list_turn[n_train:-1]\n",
    "    test_labels = label_list_turn[n_train:-1]\n",
    "    test_labels = [int(float(i)) for i in test_labels]\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成batch\n",
    "#step1：将上面生成的List传入get_batch() ，转换类型，产生一个输入队列queue，因为img和lab\n",
    "#是分开的，所以使用tf.train.slice_input_producer()，然后用tf.read_file()从队列中读取图像\n",
    "#   image_W, image_H, ：设置好固定的图像高度和宽度\n",
    "#   设置batch_size：每个batch要放多少张图片\n",
    "#   capacity：一个队列最大多少\n",
    "def get_batch(image, label, image_W, image_H, batch_size, capacity):\n",
    "    #转换类型\n",
    "    image = tf.cast(image, tf.string)\n",
    "    label = tf.cast(label, tf.int32)\n",
    " \n",
    "    # make an input queue\n",
    "    input_queue = tf.train.slice_input_producer([image, label])\n",
    " \n",
    "    label = input_queue[1]\n",
    "    image_contents = tf.read_file(input_queue[0]) #read img from a queue  \n",
    "    \n",
    "#step2：将图像解码，不同类型的图像不能混在一起，要么只用jpeg，要么只用png等。\n",
    "    image = tf.image.decode_jpeg(image_contents, channels=3) \n",
    "    \n",
    "#step3：数据预处理，对图像进行旋转、缩放、裁剪、归一化等操作，让计算出的模型更健壮。\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, image_W, image_H)\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    \n",
    "#step4：生成batch\n",
    "#image_batch: 4D tensor [batch_size, width, height, 3],dtype=tf.float32 \n",
    "#label_batch: 1D tensor [batch_size], dtype=tf.int32\n",
    "    image_batch, label_batch = tf.train.batch([image, label],\n",
    "                                                batch_size= batch_size,\n",
    "                                                num_threads= 32, \n",
    "                                                capacity = capacity)\n",
    "    #重新排列label，行数为[batch_size]\n",
    "    label_batch = tf.reshape(label_batch, [batch_size])\n",
    "    image_batch = tf.cast(image_batch, tf.float32)\n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images, batch_size, n_classes):\n",
    "#一个简单的卷积神经网络，卷积+池化层x2，全连接层x2，最后一个softmax层做分类。\n",
    "#卷积层1\n",
    "#64个3x3的卷积核（3通道），padding=’SAME’，表示padding后卷积的图与原图尺寸一致，激活函数relu()\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        \n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[3,3,3,64], stddev = 1.0, dtype = tf.float32), \n",
    "                              name = 'weights', dtype = tf.float32)\n",
    "        \n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [64]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "        \n",
    "        conv = tf.nn.conv2d(images, weights, strides=[1,1,1,1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name= scope.name)\n",
    "        \n",
    "#池化层1\n",
    "#3x3最大池化，步长strides为2，池化后执行lrn()操作，局部响应归一化，对训练有利。\n",
    "    with tf.variable_scope('pooling1_lrn') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME', name='pooling1')\n",
    "        '''tf.nn.max_pool(value, ksize, strides, padding, name=None)\n",
    "            value :conv1 一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape\n",
    "            ksize :因为我们不想在batch和channels上做池化，所以这两个维度设为了1\n",
    "            strides :窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]\n",
    "            padding : SAME / VALID ,0补全/不补全\n",
    "            返回 feature map\n",
    "            池化目的：筛选最大值，不改变矩阵形状\n",
    "        '''\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='norm1')\n",
    "        \n",
    "#卷积层2\n",
    "#16个3x3的卷积核（16通道），padding=’SAME’，表示padding后卷积的图与原图尺寸一致，激活函数relu()\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[3,3,64,16], stddev = 0.1, dtype = tf.float32), \n",
    "                              name = 'weights', dtype = tf.float32)\n",
    "        \n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [16]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "        \n",
    "        conv = tf.nn.conv2d(norm1, weights, strides = [1,1,1,1],padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
    "        \n",
    "#池化层2\n",
    "#3x3最大池化，步长strides为2，池化后执行lrn()操作，\n",
    "    #pool2 and norm2\n",
    "    with tf.variable_scope('pooling2_lrn') as scope:\n",
    "        norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0,beta=0.75,name='norm2')\n",
    "        pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,1,1,1],padding='SAME',name='pooling2')\n",
    "        \n",
    "#全连接层3\n",
    "#128个神经元，将之前pool层的输出reshape成一行，激活函数relu()\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        reshape = tf.reshape(pool2, shape=[batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[dim,128], stddev = 0.005, dtype = tf.float32),\n",
    "                             name = 'weights', dtype = tf.float32)\n",
    "        \n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [128]), \n",
    "                             name = 'biases', dtype=tf.float32)\n",
    "        \n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "        \n",
    "#全连接层4\n",
    "#128个神经元，激活函数relu() \n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[128,128], stddev = 0.005, dtype = tf.float32),\n",
    "                              name = 'weights',dtype = tf.float32)\n",
    "        \n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [128]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "        \n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name='local4')\n",
    "        \n",
    "#dropout层        \n",
    "#    with tf.variable_scope('dropout') as scope:\n",
    "#        drop_out = tf.nn.dropout(local4, 0.8)\n",
    "            \n",
    "        \n",
    "#Softmax回归层\n",
    "#将前面的FC层输出，做一个线性回归，计算出每一类的得分，在这里是2类，所以这个层输出的是两个得分。\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[128, n_classes], stddev = 0.005, dtype = tf.float32),\n",
    "                              name = 'softmax_linear', dtype = tf.float32)\n",
    "        \n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [n_classes]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "        \n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name='softmax_linear')\n",
    " \n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss计算\n",
    "    #传入参数：logits，网络计算输出值。labels，真实值，在这里是0或者1\n",
    "    #返回参数：loss，损失值\n",
    "def losses(logits, labels):\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        cross_entropy =tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='xentropy_per_example')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope.name+'/loss', loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss损失值优化\n",
    "    #输入参数：loss。learning_rate，学习速率。\n",
    "    #返回参数：train_op，训练op，这个参数要输入sess.run中让模型去训练。\n",
    "def trainning(loss, learning_rate):\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step= global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评价/准确率计算\n",
    "    #输入参数：logits，网络计算值。labels，标签，也就是真实值，在这里是0或者1。\n",
    "    #返回参数：accuracy，当前step的平均准确率，也就是在这些batch中多少张图片被正确分类了。\n",
    "def evaluation(logits, labels):\n",
    "    with tf.variable_scope('accuracy') as scope:\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        tf.summary.scalar(scope.name+'/accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, train loss = 1.61, train accuracy = 18.75%\n",
      "Step 10, train loss = 1.59, train accuracy = 25.00%\n",
      "Step 20, train loss = 1.56, train accuracy = 26.56%\n",
      "Step 30, train loss = 1.49, train accuracy = 46.88%\n",
      "Step 40, train loss = 1.38, train accuracy = 40.62%\n",
      "Step 50, train loss = 1.18, train accuracy = 48.44%\n",
      "Step 60, train loss = 1.24, train accuracy = 46.88%\n",
      "Step 70, train loss = 1.04, train accuracy = 59.38%\n",
      "Step 80, train loss = 1.10, train accuracy = 56.25%\n",
      "Step 90, train loss = 0.99, train accuracy = 56.25%\n",
      "Step 100, train loss = 1.09, train accuracy = 56.25%\n",
      "Step 110, train loss = 1.22, train accuracy = 43.75%\n",
      "Step 120, train loss = 1.12, train accuracy = 56.25%\n",
      "Step 130, train loss = 1.17, train accuracy = 65.62%\n",
      "Step 140, train loss = 1.10, train accuracy = 51.56%\n",
      "Step 150, train loss = 1.15, train accuracy = 53.12%\n",
      "Step 160, train loss = 1.21, train accuracy = 45.31%\n",
      "Step 170, train loss = 1.02, train accuracy = 62.50%\n",
      "Step 180, train loss = 1.00, train accuracy = 51.56%\n",
      "Step 190, train loss = 1.06, train accuracy = 56.25%\n",
      "Step 200, train loss = 0.99, train accuracy = 59.38%\n",
      "Step 210, train loss = 1.01, train accuracy = 57.81%\n",
      "Step 220, train loss = 0.88, train accuracy = 62.50%\n",
      "Step 230, train loss = 0.99, train accuracy = 53.12%\n",
      "Step 240, train loss = 0.96, train accuracy = 60.94%\n",
      "Step 250, train loss = 0.88, train accuracy = 68.75%\n",
      "Step 260, train loss = 0.94, train accuracy = 70.31%\n",
      "Step 270, train loss = 0.98, train accuracy = 62.50%\n",
      "Step 280, train loss = 0.96, train accuracy = 56.25%\n",
      "Step 290, train loss = 1.20, train accuracy = 45.31%\n",
      "Step 300, train loss = 1.03, train accuracy = 60.94%\n",
      "Step 310, train loss = 0.90, train accuracy = 67.19%\n",
      "Step 320, train loss = 0.90, train accuracy = 70.31%\n",
      "Step 330, train loss = 0.96, train accuracy = 57.81%\n",
      "Step 340, train loss = 0.94, train accuracy = 65.62%\n",
      "Step 350, train loss = 0.82, train accuracy = 64.06%\n",
      "Step 360, train loss = 0.97, train accuracy = 60.94%\n",
      "Step 370, train loss = 0.92, train accuracy = 65.62%\n",
      "Step 380, train loss = 0.89, train accuracy = 68.75%\n",
      "Step 390, train loss = 0.89, train accuracy = 62.50%\n",
      "Step 400, train loss = 0.71, train accuracy = 70.31%\n",
      "Step 410, train loss = 0.95, train accuracy = 64.06%\n",
      "Step 420, train loss = 0.76, train accuracy = 71.88%\n",
      "Step 430, train loss = 0.72, train accuracy = 73.44%\n",
      "Step 440, train loss = 0.74, train accuracy = 71.88%\n",
      "Step 450, train loss = 0.76, train accuracy = 70.31%\n",
      "Step 460, train loss = 0.72, train accuracy = 75.00%\n",
      "Step 470, train loss = 0.58, train accuracy = 73.44%\n",
      "Step 480, train loss = 0.77, train accuracy = 71.88%\n",
      "Step 490, train loss = 0.63, train accuracy = 68.75%\n",
      "Step 500, train loss = 0.74, train accuracy = 71.88%\n",
      "Step 510, train loss = 0.58, train accuracy = 75.00%\n",
      "Step 520, train loss = 0.65, train accuracy = 75.00%\n",
      "Step 530, train loss = 0.74, train accuracy = 68.75%\n",
      "Step 540, train loss = 0.62, train accuracy = 76.56%\n",
      "Step 550, train loss = 0.70, train accuracy = 75.00%\n",
      "Step 560, train loss = 0.78, train accuracy = 70.31%\n",
      "Step 570, train loss = 0.53, train accuracy = 78.12%\n",
      "Step 580, train loss = 0.54, train accuracy = 79.69%\n",
      "Step 590, train loss = 0.71, train accuracy = 70.31%\n",
      "Step 600, train loss = 0.49, train accuracy = 81.25%\n",
      "Step 610, train loss = 0.61, train accuracy = 78.12%\n",
      "Step 620, train loss = 0.43, train accuracy = 81.25%\n",
      "Step 630, train loss = 0.66, train accuracy = 75.00%\n",
      "Step 640, train loss = 0.59, train accuracy = 75.00%\n",
      "Step 650, train loss = 0.45, train accuracy = 79.69%\n",
      "Step 660, train loss = 0.43, train accuracy = 89.06%\n",
      "Step 670, train loss = 0.51, train accuracy = 79.69%\n",
      "Step 680, train loss = 0.46, train accuracy = 87.50%\n",
      "Step 690, train loss = 0.44, train accuracy = 85.94%\n",
      "Step 700, train loss = 0.47, train accuracy = 84.38%\n",
      "Step 710, train loss = 0.28, train accuracy = 96.88%\n",
      "Step 720, train loss = 0.32, train accuracy = 93.75%\n",
      "Step 730, train loss = 0.37, train accuracy = 84.38%\n",
      "Step 740, train loss = 0.41, train accuracy = 85.94%\n",
      "Step 750, train loss = 0.32, train accuracy = 92.19%\n",
      "Step 760, train loss = 0.56, train accuracy = 82.81%\n",
      "Step 770, train loss = 0.39, train accuracy = 89.06%\n",
      "Step 780, train loss = 0.36, train accuracy = 87.50%\n",
      "Step 790, train loss = 0.30, train accuracy = 93.75%\n",
      "Step 800, train loss = 0.30, train accuracy = 89.06%\n",
      "Step 810, train loss = 0.47, train accuracy = 82.81%\n",
      "Step 820, train loss = 0.41, train accuracy = 89.06%\n",
      "Step 830, train loss = 0.43, train accuracy = 85.94%\n",
      "Step 840, train loss = 0.26, train accuracy = 90.62%\n",
      "Step 850, train loss = 0.32, train accuracy = 92.19%\n",
      "Step 860, train loss = 0.36, train accuracy = 84.38%\n",
      "Step 870, train loss = 0.20, train accuracy = 96.88%\n",
      "Step 880, train loss = 0.22, train accuracy = 90.62%\n",
      "Step 890, train loss = 0.35, train accuracy = 87.50%\n"
     ]
    }
   ],
   "source": [
    "#变量声明\n",
    "N_CLASSES = 5  #husky,jiwawa,poodle,qiutian\n",
    "IMG_W = 64   # resize图像，太大的话训练时间久\n",
    "IMG_H = 64\n",
    "BATCH_SIZE =64\n",
    "CAPACITY = 200\n",
    "MAX_STEP = 900 # 一般大于10K\n",
    "learning_rate = 0.0001 # 一般小于0.0001\n",
    " \n",
    "#获取批次batch\n",
    "train_dir = 'F:/Desktop/TF图集/flower_train_1/output/'   #训练样本的读入路径\n",
    "logs_train_dir = 'F:/Desktop/TF图集/flower_train_1/log/'    #logs存储路径\n",
    "\n",
    "#train, train_label = input_data.get_files(train_dir)\n",
    "train, train_label, val, val_label = get_files(train_dir, 0.3)\n",
    "#训练数据及标签\n",
    "train_batch,train_label_batch = get_batch(train, train_label, IMG_W, IMG_H, BATCH_SIZE, CAPACITY)\n",
    "#测试数据及标签\n",
    "val_batch, val_label_batch = get_batch(val, val_label, IMG_W, IMG_H, BATCH_SIZE, CAPACITY) \n",
    " \n",
    "#训练操作定义\n",
    "train_logits = inference(train_batch, BATCH_SIZE, N_CLASSES)\n",
    "train_loss = losses(train_logits, train_label_batch)        \n",
    "train_op = trainning(train_loss, learning_rate)\n",
    "train_acc = evaluation(train_logits, train_label_batch)\n",
    " \n",
    "#测试操作定义\n",
    "test_logits = inference(val_batch, BATCH_SIZE, N_CLASSES)\n",
    "test_loss = losses(test_logits, val_label_batch)        \n",
    "test_acc = evaluation(test_logits, val_label_batch)\n",
    " \n",
    "#这个是log汇总记录\n",
    "summary_op = tf.summary.merge_all() \n",
    " \n",
    "#产生一个会话\n",
    "with tf.Session() as sess:\n",
    "    #产生一个writer来写log文件\n",
    "    train_writer = tf.summary.FileWriter(logs_train_dir, sess.graph) \n",
    "    #val_writer = tf.summary.FileWriter(logs_test_dir, sess.graph) \n",
    "    #产生一个saver来存储训练好的模型\n",
    "    saver = tf.train.Saver()\n",
    "    #所有节点初始化\n",
    "    sess.run(tf.global_variables_initializer())  \n",
    "    #队列监控\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    #进行batch的训练\n",
    "    try:\n",
    "        #执行MAX_STEP步的训练，一步一个batch\n",
    "        for step in np.arange(MAX_STEP):\n",
    "            if coord.should_stop():\n",
    "                break\n",
    "            #启动以下操作节点，有个疑问，为什么train_logits在这里没有开启？\n",
    "            _, tra_loss, tra_acc = sess.run([train_op, train_loss, train_acc])\n",
    "\n",
    "            #每隔50步打印一次当前的loss以及acc，同时记录log，写入writer   \n",
    "            if step % 10  == 0:\n",
    "                print('Step %d, train loss = %.2f, train accuracy = %.2f%%' %(step, tra_loss, tra_acc*100.0))\n",
    "                summary_str = sess.run(summary_op)\n",
    "                train_writer.add_summary(summary_str, step)\n",
    "            #每隔100步，保存一次训练好的模型\n",
    "            if (step + 1) == MAX_STEP:\n",
    "                checkpoint_path = os.path.join(logs_train_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step=step)\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training -- epoch limit reached')\n",
    "\n",
    "    finally:\n",
    "        coord.request_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
